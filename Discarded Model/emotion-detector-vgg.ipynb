{"cells":[{"metadata":{"_uuid":"72efa413-e620-4cd1-a8dc-f4fd7d51a2ec","_cell_guid":"fb7c2aaf-783c-455e-aeca-16e741ec0205","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#importing all of our libraries and data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport keras\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom tensorflow.keras.applications import VGG16, InceptionResNetV2, ResNet50, Xception","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#passing the path of the directory containing training and testing data\ntrain_dir = \"../input/fer2013/train\"\ntest_dir = \"../input/fer2013/test\"","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 48 #defining the size of our image","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#performing data augmentation on our data\ntrain_datagen = image.ImageDataGenerator(#rotation_range = 180,\n                                         width_shift_range = 0.1,\n                                         height_shift_range = 0.1,\n                                         #brightness_range = [0.1,1.1],\n                                         horizontal_flip = True,\n                                         #vertical_flip = True,\n                                         rescale = 1./255,\n                                         zoom_range = 0.2,\n                                         validation_split = 0.2\n                                        )\nvalidation_datagen = image.ImageDataGenerator(rescale = 1./255,\n                                             validation_split = 0.2)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#taking the path of the directory and generating batches of our augmented data\ntrain_generator = train_datagen.flow_from_directory(directory = train_dir,\n                                                               target_size = (IMG_SIZE,IMG_SIZE),\n                                                               color_mode = \"grayscale\",\n                                                               class_mode = \"categorical\",\n                                                               batch_size = 64,\n                                                               #shuffle = False,\n                                                               subset = \"training\"\n)\n\nvalidation_generator = validation_datagen.flow_from_directory(directory = test_dir,\n                                                             target_size = (IMG_SIZE,IMG_SIZE),\n                                                             color_mode = \"grayscale\",\n                                                             class_mode = \"categorical\",\n                                                             batch_size = 64,\n                                                             #shuffle = True,\n                                                             subset = \"validation\"\n)","execution_count":7,"outputs":[{"output_type":"stream","text":"Found 22968 images belonging to 7 classes.\nFound 1432 images belonging to 7 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = tf.keras.Sequential()\n#model.add(VGG16(include_top = False,weights = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',input_shape= (IMG_SIZE,IMG_SIZE,3)))\n#model.add(MaxPool2D(padding = 'same'))\n#model.add(BatchNormalization(axis=-1))\n#model.add(Flatten())\n#model.add(Dense(7,activation = 'softmax'))","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64,(3,3),activation = 'relu',input_shape = (IMG_SIZE,IMG_SIZE,1),padding = 'same'),\n    tf.keras.layers.Conv2D(64,(3,3),activation = 'relu',padding = 'same'),\n    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Conv2D(128,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.Conv2D(128,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Conv2D(256,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.Conv2D(256,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.Conv2D(256,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Conv2D(512,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.Conv2D(512,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.Conv2D(512,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Conv2D(512,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.Conv2D(512,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.Conv2D(512,(3,3),activation= 'relu',padding = 'same'),\n    tf.keras.layers.MaxPool2D(pool_size=2,strides=2),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4096,activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(4096,activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(7,activation = 'softmax')\n])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax\noptimizer = Adam(lr = 0.0001)\nmodel.compile(loss = \"categorical_crossentropy\",optimizer = optimizer,metrics = ['accuracy'])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30\nbatch_size = 64","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":12,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 48, 48, 64)        640       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 48, 48, 64)        36928     \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 24, 24, 64)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 24, 24, 128)       73856     \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 24, 24, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 12, 12, 128)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 12, 12, 256)       295168    \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 12, 12, 256)       590080    \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 12, 12, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 6, 6, 256)         0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 6, 6, 512)         1180160   \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 6, 6, 512)         2359808   \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 6, 6, 512)         2359808   \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 3, 3, 512)         0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 3, 3, 512)         2359808   \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 3, 3, 512)         2359808   \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 3, 3, 512)         2359808   \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 1, 1, 512)         0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 1, 1, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 4096)              2101248   \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 4096)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 4096)              16781312  \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 4096)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 7)                 28679     \n=================================================================\nTotal params: 33,624,775\nTrainable params: 33,624,775\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch =22968//64, epochs = epochs,validation_data = validation_generator,validation_steps = 1432//64)","execution_count":36,"outputs":[{"output_type":"stream","text":"Epoch 1/30\n358/358 [==============================] - 35s 95ms/step - loss: 1.8927 - accuracy: 0.2447 - val_loss: 1.8244 - val_accuracy: 0.2464\nEpoch 2/30\n358/358 [==============================] - 33s 93ms/step - loss: 1.8496 - accuracy: 0.2618 - val_loss: 1.8183 - val_accuracy: 0.2450\nEpoch 3/30\n358/358 [==============================] - 33s 93ms/step - loss: 1.8191 - accuracy: 0.2466 - val_loss: 1.8149 - val_accuracy: 0.2486\nEpoch 4/30\n358/358 [==============================] - 33s 92ms/step - loss: 1.8278 - accuracy: 0.2489 - val_loss: 1.8232 - val_accuracy: 0.2479\nEpoch 5/30\n358/358 [==============================] - 34s 94ms/step - loss: 1.8315 - accuracy: 0.2143 - val_loss: 1.8146 - val_accuracy: 0.2479\nEpoch 6/30\n358/358 [==============================] - 33s 93ms/step - loss: 1.8235 - accuracy: 0.2390 - val_loss: 1.8163 - val_accuracy: 0.2464\nEpoch 7/30\n358/358 [==============================] - 34s 95ms/step - loss: 1.7868 - accuracy: 0.2831 - val_loss: 1.8124 - val_accuracy: 0.2486\nEpoch 8/30\n358/358 [==============================] - 34s 95ms/step - loss: 1.7937 - accuracy: 0.2853 - val_loss: 1.8129 - val_accuracy: 0.2493\nEpoch 9/30\n358/358 [==============================] - 35s 96ms/step - loss: 1.8143 - accuracy: 0.2563 - val_loss: 1.8141 - val_accuracy: 0.2493\nEpoch 10/30\n358/358 [==============================] - 34s 95ms/step - loss: 1.8519 - accuracy: 0.2373 - val_loss: 1.8172 - val_accuracy: 0.2436\nEpoch 11/30\n358/358 [==============================] - 34s 96ms/step - loss: 1.7916 - accuracy: 0.2774 - val_loss: 1.8156 - val_accuracy: 0.2464\nEpoch 12/30\n358/358 [==============================] - 34s 95ms/step - loss: 1.8222 - accuracy: 0.2701 - val_loss: 1.8121 - val_accuracy: 0.2479\nEpoch 13/30\n358/358 [==============================] - 34s 94ms/step - loss: 1.8278 - accuracy: 0.2393 - val_loss: 1.8104 - val_accuracy: 0.2500\nEpoch 14/30\n358/358 [==============================] - 33s 93ms/step - loss: 1.8227 - accuracy: 0.2750 - val_loss: 1.8136 - val_accuracy: 0.2479\nEpoch 15/30\n358/358 [==============================] - 34s 94ms/step - loss: 1.7933 - accuracy: 0.2812 - val_loss: 1.8196 - val_accuracy: 0.2472\nEpoch 16/30\n358/358 [==============================] - 33s 91ms/step - loss: 1.8164 - accuracy: 0.2530 - val_loss: 1.8124 - val_accuracy: 0.2493\nEpoch 17/30\n358/358 [==============================] - 33s 92ms/step - loss: 1.8193 - accuracy: 0.2584 - val_loss: 1.8115 - val_accuracy: 0.2472\nEpoch 18/30\n358/358 [==============================] - 33s 93ms/step - loss: 1.8354 - accuracy: 0.2590 - val_loss: 1.8122 - val_accuracy: 0.2479\nEpoch 19/30\n358/358 [==============================] - 33s 92ms/step - loss: 1.7902 - accuracy: 0.2776 - val_loss: 1.8133 - val_accuracy: 0.2486\nEpoch 20/30\n358/358 [==============================] - 33s 91ms/step - loss: 1.8257 - accuracy: 0.2569 - val_loss: 1.8138 - val_accuracy: 0.2479\nEpoch 21/30\n358/358 [==============================] - 33s 91ms/step - loss: 1.8128 - accuracy: 0.2202 - val_loss: 1.8156 - val_accuracy: 0.2479\nEpoch 22/30\n358/358 [==============================] - 33s 91ms/step - loss: 1.8257 - accuracy: 0.2360 - val_loss: 1.8139 - val_accuracy: 0.2457\nEpoch 23/30\n358/358 [==============================] - 33s 92ms/step - loss: 1.8030 - accuracy: 0.2513 - val_loss: 1.8147 - val_accuracy: 0.2486\nEpoch 24/30\n358/358 [==============================] - 33s 92ms/step - loss: 1.8138 - accuracy: 0.2605 - val_loss: 1.8145 - val_accuracy: 0.2464\nEpoch 25/30\n358/358 [==============================] - 34s 94ms/step - loss: 1.8172 - accuracy: 0.2480 - val_loss: 1.8075 - val_accuracy: 0.2486\nEpoch 26/30\n358/358 [==============================] - 33s 93ms/step - loss: 1.8281 - accuracy: 0.2263 - val_loss: 1.8143 - val_accuracy: 0.2479\nEpoch 27/30\n358/358 [==============================] - 33s 93ms/step - loss: 1.8454 - accuracy: 0.2339 - val_loss: 1.8127 - val_accuracy: 0.2486\nEpoch 28/30\n358/358 [==============================] - 33s 92ms/step - loss: 1.8039 - accuracy: 0.2414 - val_loss: 1.8157 - val_accuracy: 0.2450\nEpoch 29/30\n358/358 [==============================] - 33s 91ms/step - loss: 1.8027 - accuracy: 0.2405 - val_loss: 1.8142 - val_accuracy: 0.2472\nEpoch 30/30\n358/358 [==============================] - 33s 92ms/step - loss: 1.8116 - accuracy: 0.2810 - val_loss: 1.8133 - val_accuracy: 0.2479\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}